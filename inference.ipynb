{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'yolov8m.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-17 01:49:37--  https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/85d133fa-daee-4e88-bd02-ae165ee0d196?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240416T201941Z&X-Amz-Expires=300&X-Amz-Signature=f56a8909ce1167b76e9c2e72d6985c3cd1b5e8b0cd55b30b2827de2edde5bca0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=521807533&response-content-disposition=attachment%3B%20filename%3Dyolov8m.pt&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-04-17 01:49:38--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/85d133fa-daee-4e88-bd02-ae165ee0d196?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240416T201941Z&X-Amz-Expires=300&X-Amz-Signature=f56a8909ce1167b76e9c2e72d6985c3cd1b5e8b0cd55b30b2827de2edde5bca0&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=521807533&response-content-disposition=attachment%3B%20filename%3Dyolov8m.pt&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 52117635 (50M) [application/octet-stream]\n",
      "Saving to: ‘yolov8m.pt’\n",
      "\n",
      "yolov8m.pt          100%[===================>]  49.70M  4.10MB/s    in 17s     \n",
      "\n",
      "2024-04-17 01:49:57 (2.88 MB/s) - ‘yolov8m.pt’ saved [52117635/52117635]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt -O {weights_path}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from PIL import ImageFont,ImageDraw,Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8m-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(image, kpt, a, b, c, draw = True):\n",
    "    # p1, p2, p3 are the points in format [x, y]\n",
    "    # Calculate the vectors\n",
    "    p1 = kpt[a]\n",
    "    p2 = kpt[b]\n",
    "    p3 = kpt[c]\n",
    "    v1 = np.array(p1) - np.array(p2)\n",
    "    v2 = np.array(p3) - np.array(p2)\n",
    "    \n",
    "    # Calculate the angle in radians\n",
    "    angle_rad = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "    \n",
    "    # Convert to degrees\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    if angle_deg < 0:\n",
    "        angle_deg += 360\n",
    "    # draw coordinates\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    x3, y3 = p3\n",
    "    \n",
    "    if draw:\n",
    "        cv2.line(image,(int(x1),int(y1)),(int(x2),int(y2)),(50,240,250),3)\n",
    "        cv2.line(image,(int(x3),int(y3)),(int(x2),int(y2)),(50,240,250),3)\n",
    "        \n",
    "        cv2.circle(image,(int(x1),int(y1)),10,(50,240,250),cv2.FILLED)\n",
    "        cv2.circle(image,(int(x1),int(y1)),20,(50,240,250),4)\n",
    "        cv2.circle(image,(int(x2),int(y2)),10,(50,240,250),cv2.FILLED)\n",
    "        cv2.circle(image,(int(x2),int(y2)),20,(50,240,250),4)\n",
    "        cv2.circle(image,(int(x3),int(y3)),10,(50,240,250),cv2.FILLED)\n",
    "        cv2.circle(image,(int(x3),int(y3)),20,(50,240,250),4)\n",
    "\n",
    "        # cv2.imshow(\"Angle\", image)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "    \n",
    "    return angle_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_point_mapper = {\n",
    "    0: \"nose\",\n",
    "    1: \"left_eye\",\n",
    "    2: \"right_eye\",\n",
    "    3: \"left_ear\",\n",
    "    4: \"right_ear\",\n",
    "    5: \"left_shoulder\",\n",
    "    6: \"right_shoulder\",\n",
    "    7: \"left_elbow\",\n",
    "    8: \"right_elbow\",\n",
    "    9: \"left_wrist\",\n",
    "    10: \"right_wrist\",\n",
    "    11: \"left_hip\",\n",
    "    12: \"right_hip\",\n",
    "    13: \"left_knee\",\n",
    "    14: \"right_knee\",\n",
    "    15: \"left_ankle\",\n",
    "    16: \"right_ankle\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 899.5ms\n",
      "Speed: 2.8ms preprocess, 899.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "171.2667\n",
      "\n",
      "0: 480x640 1 person, 766.2ms\n",
      "Speed: 3.9ms preprocess, 766.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "170.33229\n",
      "\n",
      "0: 480x640 1 person, 496.6ms\n",
      "Speed: 1.5ms preprocess, 496.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "170.29903\n",
      "\n",
      "0: 480x640 1 person, 502.6ms\n",
      "Speed: 1.3ms preprocess, 502.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "169.80457\n",
      "\n",
      "0: 480x640 1 person, 504.8ms\n",
      "Speed: 1.3ms preprocess, 504.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "168.3439\n",
      "\n",
      "0: 480x640 1 person, 496.9ms\n",
      "Speed: 1.3ms preprocess, 496.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "166.77094\n",
      "\n",
      "0: 480x640 1 person, 507.0ms\n",
      "Speed: 1.3ms preprocess, 507.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "167.0375\n",
      "\n",
      "0: 480x640 1 person, 529.3ms\n",
      "Speed: 1.3ms preprocess, 529.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "159.50685\n",
      "\n",
      "0: 480x640 1 person, 527.5ms\n",
      "Speed: 1.4ms preprocess, 527.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "157.17159\n",
      "\n",
      "0: 480x640 1 person, 536.1ms\n",
      "Speed: 1.4ms preprocess, 536.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "153.74765\n",
      "\n",
      "0: 480x640 1 person, 528.6ms\n",
      "Speed: 1.4ms preprocess, 528.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "151.21512\n",
      "\n",
      "0: 480x640 1 person, 538.1ms\n",
      "Speed: 1.3ms preprocess, 538.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "143.25246\n",
      "\n",
      "0: 480x640 1 person, 1203.1ms\n",
      "Speed: 22.7ms preprocess, 1203.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "141.65207\n",
      "\n",
      "0: 480x640 1 person, 1731.4ms\n",
      "Speed: 6.1ms preprocess, 1731.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "139.96379\n",
      "\n",
      "0: 480x640 1 person, 1355.3ms\n",
      "Speed: 1.6ms preprocess, 1355.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "135.51027\n",
      "\n",
      "0: 480x640 1 person, 1032.5ms\n",
      "Speed: 4.8ms preprocess, 1032.5ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "127.934395\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(width, height)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m landmarks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 23\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeypoints\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, kp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keypoints\u001b[38;5;241m.\u001b[39mxy):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/engine/model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/engine/model.py:452\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/engine/predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/engine/predictor.py:248\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 248\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/engine/predictor.py:142\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/nn/autobackend.py:447\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 447\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/nn/tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/nn/tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/nn/tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    129\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/nn/modules/block.py:229\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    230\u001b[0m     y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/shooting_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "push_ups = 0\n",
    "direction = 0\n",
    "bar = 0\n",
    "cap = cv2.VideoCapture('demo.mp4')\n",
    "fontpath = \"futur.ttf\"\n",
    "font = ImageFont.truetype(fontpath,32)\n",
    "\n",
    "font1 = ImageFont.truetype(fontpath,160)\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, img = cap.read() #640 x 480\n",
    "\n",
    "\n",
    "    if ret != True:\n",
    "        break\n",
    "    width = 640\n",
    "    height = 480\n",
    "    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)\n",
    "    #Determine dimensions of video - Help with creation of box in Line 43\n",
    "    fh, fw = height, width\n",
    "    # print(width, height)\n",
    "    landmarks = []\n",
    "    results = model(img)\n",
    "    keypoints = results[0].keypoints\n",
    "    for i, kp in enumerate(keypoints.xy):\n",
    "        landmarks.append([i, kp])\n",
    "    color = (128,0,0)#color = (254,118,136)\n",
    "    for i in range(len(landmarks)):\n",
    "        kpt = landmarks[i][1]\n",
    "        #right arm = (5,7,9) , left arm = (6,8,10)\n",
    "        angleL = calculate_angle(img, kpt, 5, 7, 9, draw = True) \n",
    "        angleR = calculate_angle(img, kpt, 5, 7, 9, draw = True) \n",
    "        print(angleR)\n",
    "        \n",
    "        Percentage = np.interp(angleR,(120,150),(0,100))\n",
    "        \n",
    "\n",
    "        bar = np.interp(angleR,(130,150),(int(fh)-100,100))\n",
    "        \n",
    "\n",
    "        #check for pushup press\n",
    "        if direction == 0:\n",
    "            if Percentage == 100:\n",
    "                \n",
    "                push_ups += 0.5\n",
    "                \n",
    "                direction = 1\n",
    "            \n",
    "                \n",
    "        if direction == 1:\n",
    "            if Percentage == 0:\n",
    "                \n",
    "                push_ups += 0.5\n",
    "                \n",
    "                direction = 0\n",
    "        \n",
    "        cv2.line(img,(100,100),(100,int(fh)-100),(128,128,128),30)\n",
    "        cv2.line(img,(100,int(bar)),(100, int(fh)-100),color,30)\n",
    "        \n",
    "        if (int(Percentage) < 10):\n",
    "            cv2.line(img,(155,int(bar)),(190,int(bar)),color,40)\n",
    "        elif ((int(Percentage) >= 10) and (int(Percentage) < 100)):\n",
    "            cv2.line(img,(155,int(bar)),(200,int(bar)),color,40)\n",
    "        else:\n",
    "            cv2.line(img,(155,int(bar)),(210,int(bar)),color,40)\n",
    "    im = Image.fromarray(img)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    #draw.rounded_rectangle((fw-300,(fh//2)+100 , fw-50,(fh//2)+100),fill = color,radius = 40)\n",
    "    \n",
    "    draw.text((145,int(bar)-17),f\"{int(Percentage)}%\",font=font, fill= (255,255,255))\n",
    "        \n",
    "    draw.text((fw-200,(fh//2)-250),f\"{int(push_ups)}\",font=font1, fill= (128,0,0))\n",
    "                    \n",
    "    img_ = img.copy()\n",
    "    img_= cv2.resize(img_,(960,540),interpolation = cv2.INTER_LINEAR)\n",
    "    cv2.imshow('detection',img_)\n",
    "    cv2.waitKey(1)\n",
    "print(f'final {push_ups=}')\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.62162671637"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 819.5ms\n",
      "Speed: 6.9ms preprocess, 819.5ms inference, 1874.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 1569.7ms\n",
      "Speed: 23.1ms preprocess, 1569.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 600.1ms\n",
      "Speed: 1.8ms preprocess, 600.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 582.0ms\n",
      "Speed: 1.4ms preprocess, 582.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 510.3ms\n",
      "Speed: 1.3ms preprocess, 510.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 513.7ms\n",
      "Speed: 1.3ms preprocess, 513.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 510.2ms\n",
      "Speed: 1.9ms preprocess, 510.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 526.0ms\n",
      "Speed: 2.5ms preprocess, 526.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 516.4ms\n",
      "Speed: 1.3ms preprocess, 516.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 528.9ms\n",
      "Speed: 1.8ms preprocess, 528.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 514.6ms\n",
      "Speed: 1.2ms preprocess, 514.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 519.8ms\n",
      "Speed: 2.3ms preprocess, 519.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 513.9ms\n",
      "Speed: 1.5ms preprocess, 513.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 511.8ms\n",
      "Speed: 1.3ms preprocess, 511.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 583.5ms\n",
      "Speed: 1.4ms preprocess, 583.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 509.2ms\n",
      "Speed: 1.3ms preprocess, 509.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 556.0ms\n",
      "Speed: 1.3ms preprocess, 556.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 517.3ms\n",
      "Speed: 1.4ms preprocess, 517.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 523.1ms\n",
      "Speed: 1.5ms preprocess, 523.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 511.2ms\n",
      "Speed: 1.4ms preprocess, 511.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 524.2ms\n",
      "Speed: 1.3ms preprocess, 524.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 513.6ms\n",
      "Speed: 1.3ms preprocess, 513.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 512.0ms\n",
      "Speed: 1.3ms preprocess, 512.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 516.1ms\n",
      "Speed: 1.3ms preprocess, 516.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 510.5ms\n",
      "Speed: 1.3ms preprocess, 510.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 510.6ms\n",
      "Speed: 1.3ms preprocess, 510.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 516.4ms\n",
      "Speed: 1.3ms preprocess, 516.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 512.8ms\n",
      "Speed: 2.4ms preprocess, 512.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 512.8ms\n",
      "Speed: 1.5ms preprocess, 512.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 510.0ms\n",
      "Speed: 1.3ms preprocess, 510.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 593.6ms\n",
      "Speed: 1.3ms preprocess, 593.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 548.9ms\n",
      "Speed: 1.6ms preprocess, 548.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 513.3ms\n",
      "Speed: 1.3ms preprocess, 513.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 530.6ms\n",
      "Speed: 2.1ms preprocess, 530.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 514.0ms\n",
      "Speed: 1.2ms preprocess, 514.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 509.5ms\n",
      "Speed: 1.3ms preprocess, 509.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 516.4ms\n",
      "Speed: 1.3ms preprocess, 516.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 523.5ms\n",
      "Speed: 1.3ms preprocess, 523.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 530.1ms\n",
      "Speed: 1.4ms preprocess, 530.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 524.3ms\n",
      "Speed: 1.3ms preprocess, 524.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 524.4ms\n",
      "Speed: 1.3ms preprocess, 524.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 521.3ms\n",
      "Speed: 1.4ms preprocess, 521.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 509.2ms\n",
      "Speed: 1.3ms preprocess, 509.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 523.5ms\n",
      "Speed: 1.3ms preprocess, 523.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 515.7ms\n",
      "Speed: 1.5ms preprocess, 515.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 520.8ms\n",
      "Speed: 1.8ms preprocess, 520.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 523.7ms\n",
      "Speed: 1.3ms preprocess, 523.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 528.0ms\n",
      "Speed: 1.2ms preprocess, 528.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 523.3ms\n",
      "Speed: 1.3ms preprocess, 523.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 526.8ms\n",
      "Speed: 1.3ms preprocess, 526.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 540.4ms\n",
      "Speed: 9.2ms preprocess, 540.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 518.3ms\n",
      "Speed: 1.4ms preprocess, 518.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 513.1ms\n",
      "Speed: 1.3ms preprocess, 513.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 511.7ms\n",
      "Speed: 2.4ms preprocess, 511.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 518.3ms\n",
      "Speed: 1.3ms preprocess, 518.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 526.3ms\n",
      "Speed: 2.2ms preprocess, 526.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 523.3ms\n",
      "Speed: 1.4ms preprocess, 523.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 522.3ms\n",
      "Speed: 1.3ms preprocess, 522.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 527.8ms\n",
      "Speed: 1.3ms preprocess, 527.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 605.6ms\n",
      "Speed: 1.5ms preprocess, 605.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 512.1ms\n",
      "Speed: 1.4ms preprocess, 512.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 517.1ms\n",
      "Speed: 1.6ms preprocess, 517.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 530.8ms\n",
      "Speed: 1.5ms preprocess, 530.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 536.4ms\n",
      "Speed: 1.9ms preprocess, 536.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 531.0ms\n",
      "Speed: 1.4ms preprocess, 531.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 525.8ms\n",
      "Speed: 1.4ms preprocess, 525.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 544.3ms\n",
      "Speed: 1.4ms preprocess, 544.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 546.5ms\n",
      "Speed: 1.4ms preprocess, 546.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 528.8ms\n",
      "Speed: 1.4ms preprocess, 528.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 563.5ms\n",
      "Speed: 1.5ms preprocess, 563.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 519.4ms\n",
      "Speed: 1.5ms preprocess, 519.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 537.0ms\n",
      "Speed: 1.3ms preprocess, 537.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 538.4ms\n",
      "Speed: 1.4ms preprocess, 538.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 525.3ms\n",
      "Speed: 1.4ms preprocess, 525.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 537.1ms\n",
      "Speed: 1.4ms preprocess, 537.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 528.4ms\n",
      "Speed: 1.4ms preprocess, 528.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 549.8ms\n",
      "Speed: 1.4ms preprocess, 549.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 527.9ms\n",
      "Speed: 1.7ms preprocess, 527.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 524.3ms\n",
      "Speed: 1.5ms preprocess, 524.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 516.9ms\n",
      "Speed: 2.1ms preprocess, 516.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 522.5ms\n",
      "Speed: 1.3ms preprocess, 522.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 520.1ms\n",
      "Speed: 1.3ms preprocess, 520.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 517.8ms\n",
      "Speed: 1.3ms preprocess, 517.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 522.4ms\n",
      "Speed: 6.4ms preprocess, 522.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 526.6ms\n",
      "Speed: 1.4ms preprocess, 526.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 528.2ms\n",
      "Speed: 1.6ms preprocess, 528.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 533.7ms\n",
      "Speed: 2.0ms preprocess, 533.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 613.1ms\n",
      "Speed: 1.4ms preprocess, 613.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 546.1ms\n",
      "Speed: 2.4ms preprocess, 546.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 526.6ms\n",
      "Speed: 2.3ms preprocess, 526.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 545.5ms\n",
      "Speed: 1.4ms preprocess, 545.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 583.9ms\n",
      "Speed: 1.4ms preprocess, 583.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 528.3ms\n",
      "Speed: 1.6ms preprocess, 528.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 530.9ms\n",
      "Speed: 2.4ms preprocess, 530.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 539.7ms\n",
      "Speed: 1.3ms preprocess, 539.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 529.0ms\n",
      "Speed: 1.4ms preprocess, 529.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 545.4ms\n",
      "Speed: 1.4ms preprocess, 545.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 525.5ms\n",
      "Speed: 2.4ms preprocess, 525.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 535.4ms\n",
      "Speed: 1.4ms preprocess, 535.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 574.0ms\n",
      "Speed: 2.7ms preprocess, 574.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 529.1ms\n",
      "Speed: 1.5ms preprocess, 529.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 537.7ms\n",
      "Speed: 2.5ms preprocess, 537.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 524.8ms\n",
      "Speed: 1.4ms preprocess, 524.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 529.0ms\n",
      "Speed: 1.4ms preprocess, 529.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 550.3ms\n",
      "Speed: 2.8ms preprocess, 550.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 537.9ms\n",
      "Speed: 2.2ms preprocess, 537.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 542.3ms\n",
      "Speed: 1.5ms preprocess, 542.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 568.1ms\n",
      "Speed: 1.6ms preprocess, 568.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 584.4ms\n",
      "Speed: 1.4ms preprocess, 584.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 543.3ms\n",
      "Speed: 1.5ms preprocess, 543.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 530.4ms\n",
      "Speed: 1.4ms preprocess, 530.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 528.9ms\n",
      "Speed: 2.0ms preprocess, 528.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 525.9ms\n",
      "Speed: 1.3ms preprocess, 525.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 529.7ms\n",
      "Speed: 1.5ms preprocess, 529.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 525.0ms\n",
      "Speed: 1.4ms preprocess, 525.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 592.4ms\n",
      "Speed: 27.7ms preprocess, 592.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 559.5ms\n",
      "Speed: 1.5ms preprocess, 559.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 555.0ms\n",
      "Speed: 3.3ms preprocess, 555.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 561.0ms\n",
      "Speed: 1.4ms preprocess, 561.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 527.3ms\n",
      "Speed: 7.3ms preprocess, 527.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 532.2ms\n",
      "Speed: 1.4ms preprocess, 532.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 543.1ms\n",
      "Speed: 1.4ms preprocess, 543.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 544.7ms\n",
      "Speed: 1.4ms preprocess, 544.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 543.7ms\n",
      "Speed: 2.5ms preprocess, 543.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n",
      "\n",
      "0: 480x640 2 persons, 544.2ms\n",
      "Speed: 1.4ms preprocess, 544.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/pose/predict\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"demo.mp4\")\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        results = model(frame, save = True)\n",
    "        annotated_frame = results[0].plot()\n",
    "        cv2.imshow(\"annotated_frame\", annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 persons, 1232.5ms\n",
      "Speed: 6.7ms preprocess, 1232.5ms inference, 27.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 645.2ms\n",
      "Speed: 2.3ms preprocess, 645.2ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 608.6ms\n",
      "Speed: 1.3ms preprocess, 608.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 584.5ms\n",
      "Speed: 1.3ms preprocess, 584.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 577.1ms\n",
      "Speed: 1.2ms preprocess, 577.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 579.3ms\n",
      "Speed: 1.3ms preprocess, 579.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 783.1ms\n",
      "Speed: 1.9ms preprocess, 783.1ms inference, 6.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 975.7ms\n",
      "Speed: 2.6ms preprocess, 975.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1045.0ms\n",
      "Speed: 1.6ms preprocess, 1045.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1002.6ms\n",
      "Speed: 4.4ms preprocess, 1002.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 607.4ms\n",
      "Speed: 1.4ms preprocess, 607.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 594.0ms\n",
      "Speed: 4.1ms preprocess, 594.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 574.9ms\n",
      "Speed: 1.2ms preprocess, 574.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 562.9ms\n",
      "Speed: 1.3ms preprocess, 562.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 556.1ms\n",
      "Speed: 1.2ms preprocess, 556.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 671.2ms\n",
      "Speed: 1.3ms preprocess, 671.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 716.2ms\n",
      "Speed: 1.5ms preprocess, 716.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 625.7ms\n",
      "Speed: 2.2ms preprocess, 625.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 712.4ms\n",
      "Speed: 1.3ms preprocess, 712.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 833.8ms\n",
      "Speed: 1.3ms preprocess, 833.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 770.8ms\n",
      "Speed: 2.9ms preprocess, 770.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 954.2ms\n",
      "Speed: 1.3ms preprocess, 954.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 667.9ms\n",
      "Speed: 1.7ms preprocess, 667.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 646.8ms\n",
      "Speed: 1.7ms preprocess, 646.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1100.6ms\n",
      "Speed: 1.3ms preprocess, 1100.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 777.0ms\n",
      "Speed: 4.8ms preprocess, 777.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 869.7ms\n",
      "Speed: 2.7ms preprocess, 869.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 991.3ms\n",
      "Speed: 1.5ms preprocess, 991.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 729.4ms\n",
      "Speed: 1.7ms preprocess, 729.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 573.1ms\n",
      "Speed: 1.3ms preprocess, 573.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 619.7ms\n",
      "Speed: 1.3ms preprocess, 619.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 601.2ms\n",
      "Speed: 1.3ms preprocess, 601.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 788.0ms\n",
      "Speed: 1.4ms preprocess, 788.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1407.1ms\n",
      "Speed: 1.4ms preprocess, 1407.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 739.6ms\n",
      "Speed: 2.4ms preprocess, 739.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 619.2ms\n",
      "Speed: 1.4ms preprocess, 619.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1205.4ms\n",
      "Speed: 1.2ms preprocess, 1205.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1697.0ms\n",
      "Speed: 1.5ms preprocess, 1697.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 7406.0ms\n",
      "Speed: 3.8ms preprocess, 7406.0ms inference, 12.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 4413.1ms\n",
      "Speed: 22.4ms preprocess, 4413.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 3452.8ms\n",
      "Speed: 2.2ms preprocess, 3452.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1643.6ms\n",
      "Speed: 1.8ms preprocess, 1643.6ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1259.0ms\n",
      "Speed: 3.1ms preprocess, 1259.0ms inference, 12.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1424.1ms\n",
      "Speed: 2.8ms preprocess, 1424.1ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1120.5ms\n",
      "Speed: 1.5ms preprocess, 1120.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1416.0ms\n",
      "Speed: 21.4ms preprocess, 1416.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 957.4ms\n",
      "Speed: 5.3ms preprocess, 957.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 4828.7ms\n",
      "Speed: 2.2ms preprocess, 4828.7ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2992.2ms\n",
      "Speed: 25.2ms preprocess, 2992.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1057.6ms\n",
      "Speed: 5.4ms preprocess, 1057.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 609.4ms\n",
      "Speed: 1.5ms preprocess, 609.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 633.7ms\n",
      "Speed: 1.4ms preprocess, 633.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 722.1ms\n",
      "Speed: 1.3ms preprocess, 722.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 768.9ms\n",
      "Speed: 1.2ms preprocess, 768.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 616.4ms\n",
      "Speed: 1.2ms preprocess, 616.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 567.0ms\n",
      "Speed: 1.9ms preprocess, 567.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 608.1ms\n",
      "Speed: 1.2ms preprocess, 608.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 636.7ms\n",
      "Speed: 1.3ms preprocess, 636.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 584.1ms\n",
      "Speed: 1.5ms preprocess, 584.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 593.8ms\n",
      "Speed: 1.3ms preprocess, 593.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 601.0ms\n",
      "Speed: 2.1ms preprocess, 601.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 545.1ms\n",
      "Speed: 1.4ms preprocess, 545.1ms inference, 6.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 530.2ms\n",
      "Speed: 1.3ms preprocess, 530.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 560.3ms\n",
      "Speed: 1.3ms preprocess, 560.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 606.4ms\n",
      "Speed: 1.3ms preprocess, 606.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 543.3ms\n",
      "Speed: 1.3ms preprocess, 543.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 548.0ms\n",
      "Speed: 1.4ms preprocess, 548.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 551.5ms\n",
      "Speed: 1.3ms preprocess, 551.5ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 897.6ms\n",
      "Speed: 2.1ms preprocess, 897.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 586.6ms\n",
      "Speed: 21.4ms preprocess, 586.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 43107.5ms\n",
      "Speed: 1.2ms preprocess, 43107.5ms inference, 33.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2061.9ms\n",
      "Speed: 24.9ms preprocess, 2061.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1752.1ms\n",
      "Speed: 8.2ms preprocess, 1752.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2158.1ms\n",
      "Speed: 6.3ms preprocess, 2158.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 908.7ms\n",
      "Speed: 1.5ms preprocess, 908.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1049.2ms\n",
      "Speed: 1.3ms preprocess, 1049.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 773.7ms\n",
      "Speed: 1.7ms preprocess, 773.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 952.7ms\n",
      "Speed: 2.9ms preprocess, 952.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 904.5ms\n",
      "Speed: 5.4ms preprocess, 904.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 790.8ms\n",
      "Speed: 1.7ms preprocess, 790.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 845.4ms\n",
      "Speed: 1.3ms preprocess, 845.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 612.3ms\n",
      "Speed: 1.6ms preprocess, 612.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 587.2ms\n",
      "Speed: 2.1ms preprocess, 587.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 546.7ms\n",
      "Speed: 1.4ms preprocess, 546.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 527.4ms\n",
      "Speed: 1.3ms preprocess, 527.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 624.2ms\n",
      "Speed: 1.5ms preprocess, 624.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 595.1ms\n",
      "Speed: 1.6ms preprocess, 595.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 647.8ms\n",
      "Speed: 1.3ms preprocess, 647.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 603.9ms\n",
      "Speed: 1.3ms preprocess, 603.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 647.0ms\n",
      "Speed: 1.3ms preprocess, 647.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 564.2ms\n",
      "Speed: 1.3ms preprocess, 564.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 582.4ms\n",
      "Speed: 1.3ms preprocess, 582.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 609.3ms\n",
      "Speed: 1.4ms preprocess, 609.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 552.7ms\n",
      "Speed: 1.3ms preprocess, 552.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 591.1ms\n",
      "Speed: 2.2ms preprocess, 591.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 620.9ms\n",
      "Speed: 1.6ms preprocess, 620.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 549.9ms\n",
      "Speed: 1.9ms preprocess, 549.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 557.6ms\n",
      "Speed: 1.3ms preprocess, 557.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 644.2ms\n",
      "Speed: 1.3ms preprocess, 644.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 558.0ms\n",
      "Speed: 3.7ms preprocess, 558.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 628.9ms\n",
      "Speed: 1.7ms preprocess, 628.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 580.6ms\n",
      "Speed: 1.5ms preprocess, 580.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1110.5ms\n",
      "Speed: 1.3ms preprocess, 1110.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 589.8ms\n",
      "Speed: 6.1ms preprocess, 589.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1190.1ms\n",
      "Speed: 3.3ms preprocess, 1190.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 641.7ms\n",
      "Speed: 2.9ms preprocess, 641.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 615.4ms\n",
      "Speed: 17.1ms preprocess, 615.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 617.1ms\n",
      "Speed: 1.3ms preprocess, 617.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 576.8ms\n",
      "Speed: 1.3ms preprocess, 576.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 547.8ms\n",
      "Speed: 1.3ms preprocess, 547.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 533.7ms\n",
      "Speed: 1.3ms preprocess, 533.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 561.0ms\n",
      "Speed: 1.6ms preprocess, 561.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 552.2ms\n",
      "Speed: 1.8ms preprocess, 552.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 628.5ms\n",
      "Speed: 1.3ms preprocess, 628.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 579.3ms\n",
      "Speed: 1.6ms preprocess, 579.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 579.0ms\n",
      "Speed: 1.3ms preprocess, 579.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 583.7ms\n",
      "Speed: 1.3ms preprocess, 583.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 537.8ms\n",
      "Speed: 1.4ms preprocess, 537.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 534.7ms\n",
      "Speed: 1.3ms preprocess, 534.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 558.7ms\n",
      "Speed: 1.4ms preprocess, 558.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 578.3ms\n",
      "Speed: 1.4ms preprocess, 578.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 548.9ms\n",
      "Speed: 1.4ms preprocess, 548.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 556.9ms\n",
      "Speed: 1.4ms preprocess, 556.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 576.7ms\n",
      "Speed: 1.9ms preprocess, 576.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 583.1ms\n",
      "Speed: 1.6ms preprocess, 583.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 579.7ms\n",
      "Speed: 1.7ms preprocess, 579.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 580.1ms\n",
      "Speed: 1.5ms preprocess, 580.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 543.8ms\n",
      "Speed: 3.1ms preprocess, 543.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 605.0ms\n",
      "Speed: 1.3ms preprocess, 605.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 557.2ms\n",
      "Speed: 1.3ms preprocess, 557.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 549.0ms\n",
      "Speed: 1.3ms preprocess, 549.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 698.0ms\n",
      "Speed: 1.7ms preprocess, 698.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 582.8ms\n",
      "Speed: 1.3ms preprocess, 582.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1797734.2ms\n",
      "Speed: 1.4ms preprocess, 1797734.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1063.8ms\n",
      "Speed: 2.1ms preprocess, 1063.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 641.6ms\n",
      "Speed: 3.5ms preprocess, 641.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 766.6ms\n",
      "Speed: 1.8ms preprocess, 766.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2138.7ms\n",
      "Speed: 2.1ms preprocess, 2138.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 795.2ms\n",
      "Speed: 1.3ms preprocess, 795.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 838.7ms\n",
      "Speed: 3.9ms preprocess, 838.7ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1074.9ms\n",
      "Speed: 5.0ms preprocess, 1074.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 808.2ms\n",
      "Speed: 1.4ms preprocess, 808.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1090.9ms\n",
      "Speed: 1.4ms preprocess, 1090.9ms inference, 8.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 865.0ms\n",
      "Speed: 1.4ms preprocess, 865.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 806.7ms\n",
      "Speed: 1.5ms preprocess, 806.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 848.1ms\n",
      "Speed: 1.5ms preprocess, 848.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 756.3ms\n",
      "Speed: 1.4ms preprocess, 756.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 667.4ms\n",
      "Speed: 1.4ms preprocess, 667.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 678.0ms\n",
      "Speed: 1.9ms preprocess, 678.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 798.3ms\n",
      "Speed: 1.5ms preprocess, 798.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 718.6ms\n",
      "Speed: 1.5ms preprocess, 718.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 670.0ms\n",
      "Speed: 2.2ms preprocess, 670.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 592.1ms\n",
      "Speed: 1.4ms preprocess, 592.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 648.2ms\n",
      "Speed: 1.5ms preprocess, 648.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 682.7ms\n",
      "Speed: 1.4ms preprocess, 682.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 544.2ms\n",
      "Speed: 1.3ms preprocess, 544.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 608.2ms\n",
      "Speed: 1.3ms preprocess, 608.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 621.0ms\n",
      "Speed: 1.9ms preprocess, 621.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 615.7ms\n",
      "Speed: 2.1ms preprocess, 615.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 679.5ms\n",
      "Speed: 1.3ms preprocess, 679.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 746.3ms\n",
      "Speed: 1.4ms preprocess, 746.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 682.7ms\n",
      "Speed: 1.4ms preprocess, 682.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 782.7ms\n",
      "Speed: 1.4ms preprocess, 782.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 698.2ms\n",
      "Speed: 1.7ms preprocess, 698.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 711.4ms\n",
      "Speed: 1.6ms preprocess, 711.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 676.3ms\n",
      "Speed: 1.4ms preprocess, 676.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 685.1ms\n",
      "Speed: 1.4ms preprocess, 685.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 631.4ms\n",
      "Speed: 1.5ms preprocess, 631.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 694.0ms\n",
      "Speed: 1.4ms preprocess, 694.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 661.8ms\n",
      "Speed: 1.5ms preprocess, 661.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 729.8ms\n",
      "Speed: 2.1ms preprocess, 729.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 709.5ms\n",
      "Speed: 1.4ms preprocess, 709.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 668.6ms\n",
      "Speed: 1.6ms preprocess, 668.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 709.6ms\n",
      "Speed: 1.3ms preprocess, 709.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 677.9ms\n",
      "Speed: 1.3ms preprocess, 677.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 625.6ms\n",
      "Speed: 1.3ms preprocess, 625.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 706.0ms\n",
      "Speed: 1.5ms preprocess, 706.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 602.1ms\n",
      "Speed: 1.3ms preprocess, 602.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 715.8ms\n",
      "Speed: 1.4ms preprocess, 715.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 654.8ms\n",
      "Speed: 3.7ms preprocess, 654.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1242.7ms\n",
      "Speed: 1.4ms preprocess, 1242.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 603.1ms\n",
      "Speed: 5.4ms preprocess, 603.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 735.2ms\n",
      "Speed: 1.3ms preprocess, 735.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 754.0ms\n",
      "Speed: 1.4ms preprocess, 754.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 676.2ms\n",
      "Speed: 1.2ms preprocess, 676.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 763.8ms\n",
      "Speed: 1.3ms preprocess, 763.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 672.2ms\n",
      "Speed: 1.4ms preprocess, 672.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 682.0ms\n",
      "Speed: 1.3ms preprocess, 682.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 766.1ms\n",
      "Speed: 1.4ms preprocess, 766.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 721.4ms\n",
      "Speed: 1.4ms preprocess, 721.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 656.3ms\n",
      "Speed: 1.4ms preprocess, 656.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 605.0ms\n",
      "Speed: 1.4ms preprocess, 605.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 648.3ms\n",
      "Speed: 2.2ms preprocess, 648.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 614.3ms\n",
      "Speed: 1.3ms preprocess, 614.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 620.4ms\n",
      "Speed: 1.6ms preprocess, 620.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1083.6ms\n",
      "Speed: 1.6ms preprocess, 1083.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 825.3ms\n",
      "Speed: 1.4ms preprocess, 825.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 665.4ms\n",
      "Speed: 1.3ms preprocess, 665.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 701.5ms\n",
      "Speed: 1.3ms preprocess, 701.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 648.6ms\n",
      "Speed: 1.2ms preprocess, 648.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 260818.7ms\n",
      "Speed: 1.4ms preprocess, 260818.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1517.8ms\n",
      "Speed: 23.7ms preprocess, 1517.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 821.3ms\n",
      "Speed: 1.7ms preprocess, 821.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 631.9ms\n",
      "Speed: 2.1ms preprocess, 631.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        results = model(frame)\n",
    "        result = results[0]\n",
    "        for box in result.boxes:\n",
    "            \n",
    "            cords = box.xyxy[0].tolist()\n",
    "            cords = [int(ele) for ele in cords]\n",
    "            label = result.names[int(box.cls[0])]\n",
    "            x1, y1, x2, y2 = cords[:4]\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"YOLOv8 Inference\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shooting_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
